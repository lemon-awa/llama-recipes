{"input":"[-1.8679 -6.7739] The authors propose SpellBERT, a pretrained model for CSC that includes graph-based extra features and does not rely on a predefined confusion set. This model uses a graph neural network to introduce radical and pinyin information as visual and phonetic features and a masked language model for pre-training. | [-1.8496 -6.8381] The authors propose to use entmax-based models to solve the 'cat got your tongue' problem in sequence-to-sequence models and apply label smoothing with Fenchel-Young losses. | [-1.8093 -6.862 ] The authors propose a novel pre-training method called MPNet that seeks to inherit the advantages of both BERT and XLNet while avoiding their limitations. Contrary to BERT, MPNet leverages the dependency among predicted tokens through permuted language modeling, and against XLNet, it reduces the position discrepancy by taking auxiliary position information as input to view a full sentence. | Convert the coordinate to text: [-1.8337 -6.7963]:","generate_predictions":" This paper proposes Cross-lingual Pre-training with Multilingual Masked Sentence Prediction (MSP), where masked tokens in one sentence are predicted based on both the masked token\u2019s surrounding context and its translation pairs. The approach is designed to effectively leverage multilingual information during pre-training.","labels":"The authors propose a system that uses a multi-label contrastive loss for fine-tuning large pre-trained language models in a multi-lingual setting for Framing Detection."}
{"input":"[ 4.6956 -1.4301] A new strategy is proposed that applies a smooth transition and interplay between filtering and relabelling noisy data as a dynamic curriculum, starting with clean data and gradually moving to noisy-labeled data with pseudo labels produced by a time-ensemble of the model and data augmentations. | [ 4.7182 -1.4342] To mitigate the issue of noisy labels, the paper proposes Soft Label Smoothing (SLS) and the Smooth Operator Framework for Teaching (SOFT). SLS assigns probabilities to multiple high-confidence classes based on the corresponding confidence while allocating a fixed lower probability to low-confidence classes. | [ 4.7284 -1.408 ] The authors propose a MUlti-Classifier (MUC) incremental learning paradigm, introducing an ensemble of auxiliary classifiers to estimate more effective regularization constraints. The paradigm is hoped to regularize the network parameters or activations when learning the next task. | Convert the coordinate to text: [ 4.6963 -1.4735]:","generate_predictions":" The authors introduce MetaShift, the first theoretically grounded model for task-agnostic meta-learning under distribution shifts, which uses meta-data as a means to estimate the distribution shift from data distributions that follow a mixture of Gaussians.","labels":"The authors propose Rank-N-Contrast (RNC), a framework that learns continuous representations for regression by contrasting samples against each other based on their rankings in the target space."}
{"input":"[13.7942 -0.0261] The authors illustrate that changing the numerical solver associated with a trained Neural ODE model may result in a performance loss, invalidating the concept of the model being a flow generated from an ODE. A critical step size for solver is found beyond which training yields a valid ODE vector field. | [13.8198 -0.0357] The authors elaborate on Second Order Neural ODEs (SONODEs) and Augmented NODEs (ANODEs), providing a methodology on how the adjoint sensitivity method can be extended to SONODEs, and proving that a first order optimization method is computationally more efficient. | [13.8008 -0.0485] The authors propose Spatial-Temporal Graph Ordinary Differential Equation Networks (STGODE) which captures spatial-temporal dynamics through a tensor-based ordinary differential equation (ODE), allowing for deeper network construction and synchronous use of spatial-temporal features. The model also considers semantical adjacency matrix in their framework to provide a more comprehensive understanding of the network. | Convert the coordinate to text: [13.8038 -0.0195]:","generate_predictions":" This paper presents a novel deep learning framework for data forecasting named Multi-Dimensional Graph Ordinary Differential Equation Networks (MDGODE). It constructs an adjacency matrix and a graph-based ordinary differential equation (ODE) system, transforming data forecasting into a process of trajectory prediction guided by an adjacency matrix and an ODE system.","labels":"The authors propose a model, Context-attended Graph ODE (CARE), to handle time-varying dynamic systems. CARE uses a context variable to model time-varying environment and constructs an encoder to initialize this context variable from historical trajectories."}
{"input":"[ 5.6434 -4.91  ] The paper presents an innovative way to acquire limits on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests. The idea is to view GNNs as expressions in a procedural tensor language that describes GNN layer computations. | [ 5.6896 -4.8078] The authors present two key innovations: they first argue that k-order networks can be more expressive than 1-WL tested networks and can distinguish between non-isomorphic graphs as good as the k-WL tests. They then suggest a reduced 2-order network containing just scaled identity operator, augmented with a single quadratic operation (matrix multiplication), to create a simpler and scalable model with 3-WL expressiveness. | [ 5.6921 -4.9179] A novel graph indexing and searching algorithm is proposed, using the property that Mobius transformation introduces an isomorphism between a subgraph of l^2-Delaunay graph and Delaunay graph for inner product. | Convert the coordinate to text: [ 5.6406 -4.853 ]:","generate_predictions":" The paper introduces Graph Contrastive Neural Networks (GraphCL) that contrast subgraphs with different levels of structural information within a graph with the objective to extract useful structural information in representation learning from graph data without relying on data-specific augmentations.","labels":"The authors propose a policy, GFN4Rec, a generative method that leverages the concept of flow networks to align list generation probability and its reward. It includes a log scale reward matching loss to intrinsically improve the generation diversity and an autoregressive item selection model to capture item mutual influences."}
{"input":"[ 5.638 13.786] The paper presents an off-policy, interactive RL algorithm that learns a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. It relabels all the agent's past experience when its reward model changes. | [ 5.6615 13.7966] An algorithm called Reward Query Synthesis via Trajectory Optimization (ReQueST) is proposed that safely and interactively learns a model of the user's reward function. It synthesizes hypothetical behaviors, has the user label the behaviors with rewards, and trains a neural network to predict the rewards\u2014all without interacting with the environment. | [ 5.677  13.7381] The study proposes to understand how reward hacking arises by constructing four RL environments with misspecified rewards and investigating it as a function of agent capabilities such as model capacity, action space resolution, observation space noise, and training time. | Convert the coordinate to text: [ 5.6371 13.7621]:","generate_predictions":" The authors introduce a method that simultaneously learns the rewards and dynamics to minimize cumulative costs across the distributionally-weighted trajectories. The reward and dynamics models are learned using maximum likelihood estimation along with the distributionally-weighted trajectory weighting in the estimation.","labels":"The authors propose a novel approach for curriculum RL named 'Diversify for Disagreement & Conquer (D2C)' that requires only a few examples of desired outcomes and can function in any environment."}
{"input":"[ 10.4967 -18.8439] The present study introduces the concept of distorted reconstruction from two views, which differs from traditional approaches by not satisfying the epipolar constraint, and can be computed using point correspondences and a potentially inexact estimate of the stereo configuration. | [ 10.6731 -18.8723] The authors propose a method to recover the poses of the reference plane directly from its reflections on the specular surface by establishing reflection correspondences on the reference plane in three distinct poses. | Convert the coordinate to text: [ 10.5434 -18.8509]:","generate_predictions":" A novel method, called dual-frequency illumination, is proposed which projects two high-frequency sinusoids with perpendicular directions onto a surface and captures their interference using a single camera, for 3D surface reconstruction.","labels":"Dynamo-Depth is introduced as an approach that disambiguates dynamical motion in a scene by jointly learning monocular depth, 3D independent flow field, and motion segmentation from unlabeled monocular videos."}
{"input":"[-2.6332  0.0939] This study introduces Task Adaptive Pretraining (TAPT) for Transformer-based architectures, aiming to improve the coarse binary classification of Hindi Tweets into 'Hostile' or 'Not', and multi-label classification of Tweets into 'hate', 'fake', 'offensive', and 'defamation'. | [-2.625   0.1932] The authors propose a new challenging NLP task of detecting political bias in Hindi news articles and present a transformer-based transfer learning method to fine-tune pre-trained networks for bias detection. | Convert the coordinate to text: [-2.6458  0.0878]:","generate_predictions":" The authors propose to reduce ambiguity in language model-generated text by introducing an objective function that rewards fluency and penalizes the amount of uncertainty in language model outputs, termed 'Bayesian Disentangled Multitask Language Models (BDMLMs)'.","labels":"The authors propose a fine-tuned transformer-based model for problematic webpage classification and a novel webpage data collection strategy leveraging short-text hate speech datasets. They also introduce a custom GPT-4 few-shot prompt annotation scheme for webpage annotation."}
{"input":"[ 0.2392 -9.2971] The authors propose a new task, unsupervised vision-language (VL) grammar induction, and a novel method, CLIORA, which simultaneously extracts a shared vision-language constituency tree structure grounded in both image and language modalities. | [ 0.3308 -9.313 ] The authors propose a multi-granularity retrieval system to address these challenges, which comprises of three modules - a language parsing module to extract fine-grained attributes from language descriptions, a language-augmented multi-query vehicle track retrieval module to fuse information from multiple imperfect queries, and a target vehicle attributes enhancement module to fuse static and dynamic properties of the target vehicle. | [ 0.2928 -9.3361] The authors argue that language features deserve more attention in vision-language tasks, proving this through experiments which compare different word embeddings, language models, and embedding augmentation steps on five common VL tasks. They also propose a new Graph Oriented Vision-Language Embedding (GrOVLE), derived from Word2Vec using WordNet and a unique visual-language graph based on Visual Genome. | Convert the coordinate to text: [ 0.2766 -9.259 ]:","generate_predictions":" The authors propose the idea of distilling the language-visual knowledge from a teacher model into a student model, with the aim of enabling the student model to learn more grounded visual representations and have better zero-shot transfer ability.","labels":"To address the issue of semantic misalignment, a new framework named Concept Curation (CoCu) is introduced. This uses CLIP to compensate for missing semantics in image-text pairs by maintaining a concept archive with potential visually-matched concepts."}
{"input":"[13.4506 -4.7345] A stronger weight-poisoning attack method is proposed. This method introduces a layerwise weight poisoning strategy to plant deeper undetectable backdoors with a combinatorial trigger. | [13.4317 -4.752 ] The paper proposes a novel approach, TDSSA (Truth Discovery against Strategic Sybil Attack), to defend against strategic Sybil attacks that attempt to evade traditional detection methods. | [13.4117 -4.7155] The authors develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. | Convert the coordinate to text: [13.4297 -4.7303]:","generate_predictions":" The authors propose a model-agnostic training approach, named REPAIR, to generate robust models against these data poisoning attacks by using the gradient magnitude difference of two specially designed losses inspired by data augmentation methods.","labels":"This paper introduces a robust and stealthy backdoor attack design: BITE. BITE poisons training data to establish strong correlations between a target label and certain 'trigger words' by naturally and iteratively injecting them into target-label instances."}
{"input":"[ 0.7978 -3.7405] This study explores the potential benefits of using semantic information of labels to guide the classification task in item categorization systems within e-commerce, and proposes the use of a hyperbolic space to embed product labels organized hierarchically. | [ 0.6607 -3.7885] The paper introduces self-training to SRL, utilizing abundant unlabeled tweets to lessen the dependency on annotated tweets. The authors present a novel tweet selection strategy that ensures selected tweets are both correct and informative. | Convert the coordinate to text: [ 0.7344 -3.773 ]:","generate_predictions":" The paper presents a strategy of leveraging a larger dataset with weakly-labeled information in addition to a smaller one with strong supervision. A co-learning procedure is implemented to enhance cross-domain generalizability.","labels":"The paper characterizes two ways in which ICL leverages demonstrations, namely: Task Recognition (TR) which measures the extent that LLMs can recognize a task through demonstrations and apply their pre-trained priors; and Task Learning (TL), which is the ability to capture new input-label mappings that were not seen in pre-training."}
{"input":"[-1.2245 -4.1524] The authors introduce a light hybrid retriever, which leverages an indexing-efficient dense retriever (i.e., DrBoost) and a lighter dense retriever (LITE) jointly trained on contrastive learning and knowledge distillation from DrBoost. This allows for a reduction in the indexing memory of hybrid retrievers without losing performance. | [-1.1857 -4.1053] The authors propose an entity guided question generation model that addresses these shortcomings by simultaneously capturing both the structural and sequence information in the context using a Graph Convolutional Network and a Bidirectional Long Short Term Memory Network. | [-1.1851 -4.0819] The authors propose a retrieval augmented framework, which retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. | Convert the coordinate to text: [-1.2103 -4.1702]:","generate_predictions":" The paper presents Socratic Models (SoMo), a new framework that retrieves and combines reasoning steps from an external large-scale, unstructured, and in-domain data repository to create answer explanations for a variety of QA tasks.","labels":"The authors propose an efficient algorithm for latent variable modeling to leverage large amounts of dialogue data, avoiding the need for intermediate labels. This is achieved by adapting a query generator with an off-the-shelf knowledge retriever and simultaneously training the query and response generators over the query's latent variable."}
{"input":"[2.3437 0.673 ] The authors propose a novel theoretical framework that assumes a teaching goal of minimizing the learner's expected generalization error at each iteration, and provides a theoretical justification for curriculum learning. | [2.3478 0.7919] To address this, the authors propose Data-IQ, a framework for systematically stratifying data examples into specific outcome-related subgroups (Easy, Ambiguous, Hard) based on their predictive confidence and aleatoric (data) uncertainty during training. | [2.2921 0.6725] The authors aim to determine the most important hyperparameters and their typically good values for given algorithms, using a meta-learning framework across many datasets. | Convert the coordinate to text: [2.3336 0.7327]:","generate_predictions":" The authors introduce Hyper-Consistency, a meta-learning based approach that uses model behavior to train the sampler to select highly rewarding hyperparameter configurations, and to predict which hyperparameters are most impactful and how they interact.","labels":"The authors follow a model introduced by Cai and Daskalakis to consider the case that bidders' prior distributions can be well-approximated by a topic model. They present a new approach consisting of an active learning component and a mechanism design component, in effect bridging the theory of mechanism design with the technique of Randomized Linear Algebra (RLA) for regression problems."}
{"input":"[ 1.5767 13.4959] The authors adapt three metamodels of bounded rational behavior, two based on Quantal level-k and one based on Nash equilibrium with quantal errors, to create game theoretic models of driving behavior within the context of hierarchical games, a framework used in multi-agent motion planning. | [ 1.6342 13.4793] The authors suggest an alternative approach to academic recruitment where they make a single batch of offers rather than sequential ones, treat the target number of positions as a soft constraint, and seek to maximize the overall expected value associated with candidates who accept, minus an expected penalty for deviating from the target. | [ 1.5696 13.5214] The authors present a simple model of a game where each strategy has additional costs (computational or otherwise), studying the strategic aspects of bounded rational behavior. | Convert the coordinate to text: [ 1.6075 13.5053]:","generate_predictions":" The paper studies the price of anarchy in both synchronous and asynchronous networks where each node's strategy is to set a quality to maximize its own net payoff and to invest in links with other nodes that match its quality.","labels":"The authors introduce a soft quota setting in which every post is associated with two values \u2014 a lower and upper target which denote a range for the intended number of applicants in any assignment. They allow the number of applicants assigned to fall outside this range, creating assignments with deviation."}
{"input":"[1.3273 2.4041] The authors present GenASAP, a generative model for a developed custom DNA microarray platform to survey AS levels on a large scale. | [1.3473 2.4692] The paper proposes a novel framework known as Causal Intervention by Semantic Smoothing (CISS) that aims for robustness against natural language attacks. Instead of just fitting observational data, CISS learns causal effects by smoothing in the latent semantic space to make robust predictions. | [1.3746 2.4349] The authors introduce a deep generative model-based framework, Credence, to validate causal inference methods. It generates synthetic data that closely mirror empirical distribution of the observed sample, and lets the user specify ground truth for the form and magnitude of causal effects and confounding bias. | Convert the coordinate to text: [1.3245 2.4293]:","generate_predictions":" The authors explore a broader condition for causal effect estimation when the causal graph might contain latent factors, suggesting the need for careful causal diagrammatic analysis of problem-specific structural assumptions, and the design of learning algorithms under these assumptions.","labels":"The authors theoretically clarified the relationship between SRS predictions and instance reliability and proposed two error-bounded strategies to rectify unreliable targets and input."}
{"input":"[-2.5714 -7.4227] Grounding on the connection between a pre-trained masked language model (MLM) and non-autoregressive generation on machine translation, the authors present XLM-D, a model that transforms an existing cross-lingual pre-training model into a non-autoregressive translation (NAT) model with a lightweight decorator. | [-2.5321 -7.4235] The paper introduces an empirical study of a character-level deep recurrent neural network (Char-RNN) specifically designed for Chinese text classification, that uses character-level features as input features. | Convert the coordinate to text: [-2.617  -7.4761]:","generate_predictions":" The authors propose a novel few-shot training framework known as the Language Model Fitting, which learns conditional distributions efficiently in parameter-rich language models. They also use a novel calibration method to address the over-confidence problem.","labels":"The authors propose a system consisting of a pretrained multilingual masked language model as a text encoder and a neural network as a regression model. An innovative feature is the incorporation of data augmentation via neural machine translation models to improve performance under low-resource scenarios."}
{"input":"[-2.1381 -5.3013] The paper presents ELLE, a solution for efficient lifelong pre-training for emerging data. It includes function preserved model expansion, which expands the existing PLM's width and depth for better efficiency, and pre-trained domain prompts to stimulate the proper knowledge for downstream tasks. | [-2.1822 -5.3233] The authors introduce TempLM, a system designed to distill PLMs into template-based generators to balance both faithfulness and fluency. | [-2.1849 -5.3521] This study inquires whether GPT-3 can be deployed as an effective data annotator for NLP tasks, potentially automating a human-involved process. | Convert the coordinate to text: [-2.1348 -5.3293]:","generate_predictions":" The researchers propose two simple methods to bridge the output gap between a PLM and humans, which are designed to make the model's predictions more intuitive and accessible.","labels":"The authors propose Gradient Ascent Post-training (GAP), a technique of updating pretrained language models with a few steps of gradient ascent on random, unlabeled text corpora to enhance their zero-shot generalization capabilities."}
{"input":"[12.4465  1.4814] The authors propose an improved method known as frequency domain BSS where, through the properties of linear addition and phase loss of spectrum analysis, the engineering signals are first transformed to the frequency domain, and then the spectra are processed by ICA. | [12.4595  1.4523] The authors propose a general Bayesian framework for performing ICA, which leverages ensemble learning and linear response theory from statistical physics. It can be applied to discrete and continuous sources, and also offers a way to generate new ICA algorithms without explicit prior distribution of sources. | [12.4052  1.4874] The authors pursue an alternative approach towards the identifiability of nonlinear ICA, using only assumptions on the mixing process, such as Structural Sparsity. | Convert the coordinate to text: [12.4251  1.4577]:","generate_predictions":" The authors suggest an improved method of Independent Component Analysis (ICA) that employs an autoregressive model over time of each separated source signal and its respective contribution to the original mixed signals, to separate mixed audio signals in a more robust and coherent way.","labels":"The authors propose an intermediate problem termed Causal Component Analysis (CauCA), generalizing ICA by modeling causal dependence among the latent components and presenting as a special case of CRL. This offers a focus on learning the unmixing function and the causal mechanisms."}
{"input":"[ 4.9467 -2.3852] To judge the quality of graphs and develop performance-safe GSSL methods, the authors propose a large margin separation method called LEAD for safe GSSL. The idea is that high-quality graphs should give predictive results on unlabeled data with a large margin separation, and these should be exploited while keeping the riskier small-margin graphs rarely exploited. | [ 4.9044 -2.398 ] This paper presents a Label Distribution Learning (LDL) approach to acne image analysis that considers the ambiguous information among acne severity. The authors propose a unified framework for joint acne image grading and counting, optimized by multi-task learning loss. | [ 4.9716 -2.4259] The authors propose a novel algorithm called UncertaintyAware Self-Distillation (UASD) to address the under-studied problem in SSL where there's a class distribution mismatch between labelled and unlabeled data. UASD can produce soft targets that prevent severe error propagation and enable efficient learning from unlabelled data that includes out-of-distribution samples. | Convert the coordinate to text: [ 4.926  -2.3865]:","generate_predictions":" A strategy is proposed to train a model using both a small set of unlabeled samples and an external dataset for a similar task, which can reduce the requirements of the unlabeled set.","labels":"The authors propose a novel SSL setting where unlabeled samples come from a mixed distribution that differs from the feature distribution of labeled samples, and present a solution \u2014 Self-Supervised Feature Adaptation (SSFA), a framework that adapts the feature extractor of the model to better suit the distribution of unlabeled data."}
{"input":"[ 0.2392 -9.2971] The authors propose a new task, unsupervised vision-language (VL) grammar induction, and a novel method, CLIORA, which simultaneously extracts a shared vision-language constituency tree structure grounded in both image and language modalities. | [ 0.2731 -9.359 ] The authors investigate the out-of-distribution performance of end-to-end and neuro-symbolic systems in vision-and-language reasoning tasks using four types of generalization tests including a novel segment-combine test for multi-image queries. | Convert the coordinate to text: [ 0.1561 -9.3428]:","generate_predictions":" The authors propose CLIP-It, an extension of the CLIP model, which can simultaneously segment and locate any object class given only a textual description in a fully unsupervised manner.","labels":"The authors propose a solution which first extends the textual context using word definitions contained in WordNet and in Open English WordNet. Second, it uses the CLIP model to select the most suitable image with the extended word context and additional information obtained from the BEiT image classification model."}
{"input":"[-0.0681 -9.1879] The authors study ambiguities in text-to-image generative models and propose a framework to handle these ambiguities by soliciting clarifications from the user. | [-0.048  -9.1743] To analyze the openness of CLIP-like models, the authors introduce the concept of 'extensibility', which measures the model's ability to handle new visual concepts through vocabulary expansions. | [-0.1802 -9.1464] The authors conduct a study to understand and provide guidelines for what prompt keywords and model hyperparameters can help produce coherent outputs with text-to-image generative models. | Convert the coordinate to text: [-0.107  -9.1283]:","generate_predictions":" The authors propose Text2VideoZeroShot (Text2Video-ZS), a video-text retrieval model trained in a self-supervised fashion, which uses a large corpus of text-image pairs and a small set of video-text pairs for fine-tuning and a cross-modal, time-aware mechanism to model video-text alignment temporally.","labels":"The study proposes to better understand and quantify progress in the direction of fine-grained vision-and-language understanding, by investigating four competitive V&L models on four fine-grained benchmarks."}
{"input":"[-3.2316 -5.4742] The authors propose the first unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, intending to provide insights on how to measure and improve the robustness of a model. | [-3.1683 -5.4836] This study reveals that the success of privacy attacks on language models is largely attributed to the duplication in commonly-used, web-scraped training data sets. | Convert the coordinate to text: [-3.2245 -5.4291]:","generate_predictions":" The authors propose X-FACTR, a fully unsupervised framework that uses language models to generate training data for fact-checking.","labels":"This work investigates the distractibility of large language models, that is, how model problem-solving accuracy can be affected by irrelevant context. Additionally, the authors introduce the Grade-School Math with Irrelevant Context (GSM-IC), a dataset designed to measure this distractibility"}
{"input":"[-13.2887   5.3067] The authors aim to investigate the feasibility of enabling web application development by nonprogrammers and targeting a specific subset of web applications, i.e., web-based data collection, storage, and retrieval applications. | [-13.249    5.2855] The paper presents a novel method to compile the most preferred environments in order to mitigate the issue of large space requirements induced by the existing compilation-based solutions. | [-13.3391   5.3072] The paper aims to identify the benefits that could be gained through the provision of mechanisms for extending W3 clients and examine how additional software components may be realized. | Convert the coordinate to text: [-13.2631   5.3297]:","generate_predictions":" The authors propose a new framework called the WebViz framework, with an integrated set of web-based data visualization tools designed to be reusable and scalable, with a plug-and-play web-based approach for easy usage.","labels":"The paper proposes a new approach to measure potential performance gains of upgraded Python web applications, using an interactive service that assists developers with optimizing their Python code through changes to the system infrastructure."}
{"input":"[-0.7108  0.1731] The authors propose a Neural Cognitive Diagnosis (NeuralCD) framework that uses neural networks to learn complex exercise interactions, thus providing more precise and interpretable diagnosis results. | [-0.8386  0.1529] The paper introduces a neural network-based approach, Learning Similarity Metrics (LSiM), that leverages a Siamese network architecture to calculate a robust and generalizing metric for comparison of numerical simulation field data. | Convert the coordinate to text: [-0.7326  0.0729]:","generate_predictions":" In this work, the authors propose to interpret the predictions made by a deep learning model for a given patient based on a set of predefined, clinically accurate concepts and their relationships with breast cancer. They leverage deep multiple instance learning architecture using an inductive message passing approach.","labels":"This paper presents Disentanglement based Cognitive Diagnosis (DCD), a model that works with limited exercise labels. The DCD model uses students' response records to model student proficiency, exercise difficulty, and exercise label distribution. Additionally, it introduces a group-based disentanglement and limited-labeled alignment modules to separate the factors relevant to concepts and align them with real limited labels."}
{"input":"[-2.9813 -2.5026] The authors propose GERE, a system that retrieves evidences in a generative fashion. Instead of the classical index-retrieve-rank approach, GERE generates document titles and evidence sentence identifiers to be used in fact verification. | [-2.992  -2.5288] The authors present a method that identifies and activates expert units within TLMs during inference to control the concepts in the generated output. The method can make fine-grained adjustments and can be used to correct gender bias in text generation. | [-3.0265 -2.4476] This work introduces a new method to bridge the gaps between multiple knowledge bases by producing new assertions using graph similarity computed with principal component analysis to draw analogies across various knowledge bases. | Convert the coordinate to text: [-2.941 -2.462]:","generate_predictions":" An analysis of the behavior of a general T5 pre-trained model in question answering tasks is conducted to explore the potential effects of using different numbers of training examples.","labels":"The study puts forth a new setting, actively supervised clustering for OpenRE, where clustering learning and relation labeling are alternately performed. This provides necessary clustering guidance without a significant increase in human effort. A key component of this setting involves choosing which instances to label, for which the authors develop a new strategy suitable for dynamically discovering clusters of unknown relations."}
{"input":"[-8.4974 -3.7874] The authors designed and implemented Termino, a comprehensive terminological resource for text processing that can store terminological information in a flexible, extensible relational database and execute term lookups using finite-state machines compiled from this database. | [-8.4218 -3.735 ] The paper proposes a methodology to reuse semantic structures while switching between several representation languages. In the process, the authors present a concept of semantic patterns to communicate knowledge at a certain level of representation. | [-8.4184 -3.7818] The paper proposes an innovative solution that uses a combination of lexical approaches and language games to reduce the complexity of the semantic problem by shifting from the ontology level to the simpler lexicon level and avoids the use of a centralized third party mediator. | Convert the coordinate to text: [-8.4793 -3.7511]:","generate_predictions":" The authors propose to automatically extract formal definitions from scientific text with a novel extraction method, 'DefinitionMiner', which leverages the lexico-grammatical and semantic indicators of definitions found in the scientific text.","labels":"Cornet is introduced, a system that automatically learns conditional formatting rules from user examples, efficiently eliminating the need for manual rule writing."}
{"input":"[ 3.5743 -3.9742] The authors propose a Self-supervised teaching assistant (SSTA) concept for improving transformers' performance that includes a head-level knowledge distillation method. This method mimics the attention distribution of the most important head of the supervised teacher and self-supervised teaching assistant to focus the student on the token relationships deemed by the teacher and teacher assistant. | [ 3.6242 -3.9438] This paper proposes the local correlation exploration framework for knowledge distillation. It includes modeling three kinds of local knowledge: intra-instance local relationship, inter-instance relationship on the same local position, and the inter-instance relationship across different local positions. Furthermore, a novel class-aware attention module is proposed to concentrate on informative local regions of the teacher\u2019s feature maps. | [ 3.6244 -3.942 ] The paper presents a new CLIR model trained using multi-stage knowledge distillation (KD). The model uses a teacher-student setup where the teacher is a traditional pipeline system while the student is capable of executing a single CLIR operation. | Convert the coordinate to text: [ 3.5608 -3.9359]:","generate_predictions":" This paper shows that existing methods cannot handle non-independent student representations through a quantitative analysis, and proposes a solution known as multi-head cross-attention learning (MCAL).","labels":"The paper introduces Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model, demonstrating that even smaller models can benefit from chain-of-thought prompting."}
{"input":"[-2.6697 -2.4771] The authors propose NESL (the Neuro-Episodic Schema Learner), which combines large language models and FrameNet parsing with a logical representation of language and a bootstrapping set of behavioral schemas, to learn event schemas from a continuous feed of 'situation samples' drawn from a pre-trained language model. | [-2.6684 -2.4174] The authors propose an entity class-dependent discriminative mixture model which introduces a latent entity class layer to model the correlations between entities and latent entity classes. This model can better adapt to different types of entities. | [-2.551  -2.4728] The authors propose a new method involving a latent variable model with a flexible distribution to more accurately represent the relations between entity pairs. This method also offers a unified architecture for both mention-level and pair-level relation extraction. | Convert the coordinate to text: [-2.6293 -2.4542]:","generate_predictions":"In the study, the authors present the first unified cross-stream entity alignment framework (CrossEA). This framework introduces a new cross-stream message passing mechanism that incorporates multi-stream contextualized representations of entities. The paper also introduces a new mechanism for cross-stream entity alignment within CrossEA.","labels":"The authors introduce a new dataset called DMASTE, which is manually annotated to fit real-world scenarios by providing more diverse and realistic reviews, including various lengths, diverse expressions, more aspect types, and more domains."}
{"input":"[  9.9086 -21.1698] The authors introudced an all-digital temporal compressive video camera. This camera adopts custom subsampling modes to achieve spatio-temporal multiplexing, re-establishing high-speed, full-resolution video by solving an undetermined system of equations using a sparse regularization framework. | [  9.8683 -21.1254] The authors propose coded sampling which not only allows N times temporal superresolution using N cameras but also lets in approximately N\/2 times more light compared to an equivalent high speed camera. They further introduce a new class of invertible codes that allow continuous blur in captured frames. | [  9.906  -21.2193] The authors propose Clairbuoyance, a system that provides feedback about the swimmer's orientation through lights mounted on swimming goggles, in two versions: Discrete signals relative to a chosen direction, and continuous signals providing a sense of absolute direction. | Convert the coordinate to text: [  9.8885 -21.1513]:","generate_predictions":" The authors propose a system called the Video SuperLupus which records an all-in-focus video from a sequence of multi-depth frames captured using a coded aperture camera, with each frame focused at a different depth.","labels":"The authors propose a low-light video enhancement method using hybrid inputs of events and frames, leveraging the high temporal resolution of event cameras to improve video quality in low-light conditions."}
{"input":"[8.4157 0.3889] A broad family of metric spaces that quantify representational dissimilarity is defined in this study. This involves modifying existing representational similarity measures, formulating a novel metric for convolutional layers, and identifying approximate Euclidean embeddings that can be incorporated into any off-the-shelf machine learning method. | [8.3503 0.3993] The study develops new methods for identifying representative images for a class in non-metric spaces that are more suitable than the existing condensing techniques, emphasizing that atypical points may be more important in describing classes and that the distance between two images is not a good measure of representation in non-metric spaces. | [8.3431 0.3019] This paper extends the idea of special function approximation to a class of functions, namely the discriminant functions that arise in optimal Bayesian classification of Gaussian Mixture Models (GMMs) in high dimensional spaces. They propose efficient approximation through deep neural networks. | Convert the coordinate to text: [8.3808 0.359 ]:","generate_predictions":" The paper proposes a general form of robust learning through distributionally robust optimization (DRO) and introduces the DRO++ framework for improved convergence and efficiency.","labels":"The authors introduce a linear classification model, QLDS, where the low density separation assumption is implemented via quadratic margin maximization, establishing a bridge between supervised and unsupervised learning methods."}
{"input":"[ 2.4073 15.8029] The authors propose to study the computational complexity of two problems: deciding if there is a set of nominees that allows a candidate from a given party to win the final election, and determining if a candidate from a specific party always wins, regardless of the nominees. | [ 2.4038 15.8076] The authors investigate how well certain election rules (specifically, positional scoring rules and the Minimax rule) can be approximated from partial preferences collected through either a randomized procedure (where each voter ranks a random subset of candidates) or a deterministic procedure (where each voter provides a ranking of her top candidates). | [ 2.4233 15.7984] The authors suggest measuring the proximity of a given preference profile to a nicely structured preference profile by counting the number of voters or alternatives that need to be removed to reach a nicely structured profile. | Convert the coordinate to text: [ 2.409  15.7979]:","generate_predictions":" This paper explores the existence of an integer representation for every election with a fixed number of candidates, providing an algorithm to find this representation under the condition that the number of voters is polynomial in the number of candidates.","labels":"This paper studies different proportionality axioms in relation to large classes of approval-based satisfaction functions, while considering how outcomes could be proportional with respect to more than one satisfaction function."}
